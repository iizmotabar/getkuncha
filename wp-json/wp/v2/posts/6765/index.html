{"id":6765,"date":"2021-03-05T15:28:55","date_gmt":"2021-03-05T10:28:55","guid":{"rendered":"https:\/\/marketlytics.com\/?p=6765"},"modified":"2023-05-26T10:31:01","modified_gmt":"2023-05-26T10:31:01","slug":"developing-a-gmail-to-bigquery-pipeline-using-cloud-function","status":"publish","type":"post","link":"https:\/\/marketlytics.com\/blog\/developing-a-gmail-to-bigquery-pipeline-using-cloud-function\/","title":{"rendered":"Developing a Gmail to BigQuery Pipeline Using Cloud Function"},"content":{"rendered":"\n<p>We love data. The more of it, the better. While our work is focused on digital analytics, using customer data to analyze their patterns and behaviors, this year, we decide to take it one step further and focus on <em>our own <\/em>patterns and behaviors. We realized that we have a wealth of data on how we communicate with our clients using our two main communication channels, Gmail, and Slack.<\/p>\n\n\n\n<p>These past few months, we have dived into the world of customer analytics and have worked on developing a cool data pipeline to allow us to observe our interactions with our clients in a more granular manner, using exploratory data analysis.&nbsp;<\/p>\n\n\n\n<p>In this article, we are going to guide you through the process of developing a process that takes emails from your Gmail and pushes it to <strong>BigQuery<\/strong>, a&nbsp;fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. Big fans of Google\u2019s Cloud suite, we have chosen <strong>Google Cloud Functions<\/strong>,&nbsp;a serverless execution environment for building and connecting cloud services.<\/p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Deployment<\/strong><\/h2>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Before You Begin<\/strong><\/h3>\n\n\n\n<p>Go through the following quickstart Guides to familiarize yourself with both, BigQuery and Google Cloud Functions, and enable the respective APIs,<\/p>\n\n\n\n<ul>\n<li><a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/quickstarts\/quickstart-web-ui\">BigQuery Quickstart Guide<\/a><\/li>\n\n\n\n<li><a href=\"https:\/\/cloud.google.com\/functions\/docs\/quickstart-python\">Google Cloud Function Quickstart Guide<\/a><\/li>\n<\/ul>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Setting Up BigQuery<\/strong><\/h2>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Creating a Dataset<\/strong><\/h3>\n\n\n\n<p>A <strong>dataset<\/strong> is contained within a specific project. <strong>Datasets<\/strong> are top-level containers that are used to organize and control access to your tables and views. First, you will need to set up a Dataset so you can create tables inside it. Here is how you can do it,<\/p>\n\n\n\n<ol>\n<li>Open the BigQuery page in the Cloud Console.<\/li>\n\n\n\n<li><a href=\"https:\/\/console.cloud.google.com\/bigquery\">Go to the BigQuery page<\/a><\/li>\n\n\n\n<li>In the navigation panel, in the <strong>Resources<\/strong> section, select your project.<\/li>\n\n\n\n<li>On the right side of the window, in the details panel, click <strong>Create dataset<\/strong>.<\/li>\n\n\n\n<li>On the <strong>Create dataset<\/strong> page:\n<ol>\n<li>For <strong>Dataset ID<\/strong>, enter a unique dataset <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/datasets#dataset-naming\">name<\/a>.<\/li>\n\n\n\n<li>(Optional) For <strong>Data location<\/strong>, choose a geographic <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/locations\">location<\/a> for the dataset. If you leave the value set to <strong>Default<\/strong>, the location is set to the US. After a dataset is created, the location can&#8217;t be changed.<br><br><strong>Note:<\/strong> If you choose EU or an EU-based region for the dataset location, your Core BigQuery Customer Data resides in the EU. Core BigQuery Customer Data is defined in the <a href=\"https:\/\/cloud.google.com\/terms\/service-terms#13-google-bigquery-service\">Service Specific Terms<\/a>.<\/li>\n<\/ol>\n<\/li>\n\n\n\n<li>Click <strong>Create dataset<\/strong>.<\/li>\n<\/ol>\n\n\n\n<p>You can also see <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/datasets\">Google\u2019s comprehensive guide on creating a dataset<\/a> for more details.<\/p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Creating a Table<\/strong><\/h3>\n\n\n\n<p>Next, you will need to proceed to BigQuery to set up the environment and tables where you can store your raw Gmail data. For our purpose, we have a created a <strong>dataset <\/strong>called <strong>raw_gmail<\/strong>. Inside this dataset, we are going to create a table called <strong>raw_mail_response<\/strong>.<\/p>\n\n\n\n<ol>\n<li>Open the <a href=\"https:\/\/console.cloud.google.com\/bigquery\">Google BigQuery Console<\/a>.<\/li>\n\n\n\n<li>In the left pane under Resources, find your dataset and click on it.<\/li>\n\n\n\n<li>In the right pane, you will see an option for <strong>Create Table<\/strong>. Click on this.<\/li>\n\n\n\n<li>Enter the name of the table and specify the schema as follows.<\/li>\n<\/ol>\n\n\n\n<p>We are going to use four fields for this table\u2019s schema,<\/p>\n\n\n\n<ul>\n<li><strong>event <\/strong>&#8211; String, RequiredThe event specifies the type of event &#8211; either it\u2019s a message type or a label type. For this guide, we are only going to be adding message type events.<\/li>\n\n\n\n<li><strong>action <\/strong>&#8211; String, RequiredThe action field will store whether the event is being added or removed. For this guide, we are only going to use the <em>added <\/em>action for message type events.<\/li>\n\n\n\n<li><strong>response <\/strong>&#8211; String, RequiredThe response contains the complete content &#8211; in case of a message, it contains the complete content of the message.<\/li>\n\n\n\n<li><strong>timestamp <\/strong>&#8211; String, NullableThe timestamp field stores the time the data was pushed to BigQuery so we can use it in the data processing section later on.<\/li>\n<\/ul>\n\n\n\n<p>Once you have done the configuration, click on <strong>Create table<\/strong>.<\/p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Credentials<\/strong><\/h2>\n\n\n\n<p>Now we can proceed to the Google Cloud Function section of the guide. We felt more comfortable to do this section in Python and have written the code in Python 3.8, which is triggered on a daily basis using Google Cloud Triggers. It pulls last day\u2019s messages from Gmail and pushes them to BigQuery using Google\u2019s Python client libraries.&nbsp;<\/p>\n\n\n\n<p>You can either use our code or use it as a reference to write your own code in any other language you feel comfortable in. Below, we have provided brief documentation detailing what the code does.<\/p>\n\n\n\n<p>Moreover, you will also need credentials to allow your code to access your Gmail data as well as upload the results to BigQuery. Here is how you can get those,<\/p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Getting BigQuery Credentials<\/strong><\/h3>\n\n\n\n<p>BigQuery lets you authenticate with Service Account Credentials.&nbsp;<\/p>\n\n\n\n<ol>\n<li>In the Cloud Console, go to the <strong>Create service account key<\/strong> page. <a href=\"https:\/\/console.cloud.google.com\/apis\/credentials\/serviceaccountkey\">Go to the Create Service Account Key page<\/a><\/li>\n\n\n\n<li>From the <strong>Service account<\/strong> list, select <strong>New service account<\/strong>.<\/li>\n\n\n\n<li>In the <strong>Service account name<\/strong> field, enter a name.<\/li>\n\n\n\n<li>From the <strong>Role<\/strong> list, select <strong>Project<\/strong> &gt; <strong>Owner<\/strong>.<\/li>\n\n\n\n<li>Click <strong>Create<\/strong>. A JSON file that contains your key downloads to your computer.<\/li>\n<\/ol>\n\n\n\n<p>For more details, you can visit Google\u2019s complete guide on&nbsp;<a href=\"https:\/\/cloud.google.com\/docs\/authentication\/getting-started\">Getting started with authentication<\/a>. Once you are done, you will have a JSON file in the following format,<\/p>\n\n\n\n<figure class=\"wp-block-table alignwide\"><table><tbody><tr><td><code>{<br>\"type\": \"service_account\",<br>\"project_id\": \"\",<br>\"private_key_id\": \"\",<br>\"private_key\": \"\",<br>\"client_email\": \"\",<br>\"client_id\": \"\",<br>\"auth_uri\": \"\",<br>\"token_uri\": \"\",<br>\"auth_provider_x509_cert_url\": \"\",<br>\"client_x509_cert_url\": \"\"<br>}<\/code><\/td><\/tr><\/tbody><\/table><\/figure>\n\n\n\n<p>We are going to store this file on Google Cloud Storage, which&nbsp;is a RESTful online file storage web service for storing and accessing data on Google Cloud Platform infrastructure. It allows us to safely store and access credentials without exposing them in our code files.<\/p>\n\n\n\n<p>Visit <a href=\"https:\/\/console.cloud.google.com\/storage\/browser\"><strong>Google Cloud Storage Console<\/strong><\/a><strong> <\/strong>and create a new bucket and store this file there. Make sure to use the same <strong>Google Project <\/strong>here as you will use for the Cloud Function. Copy the <strong>Path<\/strong>, you are going to need this later on.<\/p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Getting Credentials to Authenticate with Gmail<\/strong><\/h3>\n\n\n\n<p>For Gmail, you are going to need to go through the OAuth 2 workflow. For this, you will first have to use&nbsp;<a href=\"https:\/\/developers.google.com\/adwords\/api\/docs\/guides\/authentication\">Google\u2019s guide<\/a>.<\/p>\n\n\n\n<p>You can also use the Credentials from Google and generate your own <strong>tokens<\/strong>&nbsp;using the code we have written. Once you run that code on your local machine, it will open a window in your browser and prompt you to approve your application and generate the credentials file.<\/p>\n\n\n\n<p>It is provided<a href=\"https:\/\/gitlab.com\/-\/snippets\/2060409\"> over here as a GitLab snippet<\/a> as <strong>token_generator.py<\/strong>. You will need to save <strong>credentials.json, <\/strong>provided by Google, in the working directory, and run this piece of code.&nbsp;<\/p>\n\n\n\n<p>Once you run this code and approve the application, it will save the <strong>token.json <\/strong>file in your working directory.&nbsp;We are going to store this file on Google Cloud Storage as well so we can access it from our Cloud Function later. Upload this there and copy the path.<\/p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong><br>Creating Slack Webhook for Error Alerts&nbsp;<\/strong><\/h3>\n\n\n\n<p>One additional functionality that we have built-in this code is the ability to generate Slack alerts to notify in case of any error faced<\/p>\n\n\n\n<p>If you want to use it, you can create an <a href=\"https:\/\/api.slack.com\/messaging\/webhooks#create_a_webhook\">incoming Slack Webhook<\/a>&nbsp;and store the URL in the following format on the same Cloud Storage bucket being used for the credentials files.<\/p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td>{&#8220;url&#8221;:&#8221;&#8221;}<\/td><\/tr><\/tbody><\/table><\/figure>\n\n\n\n<p>Once you have created the credential files, you can move on to the actual work of creating the Google Cloud Function using the code in the <strong>main.py <\/strong>file. Below, we have provided the instructions,<\/p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Google Cloud Function<\/strong><\/h2>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Creating Function<\/strong><\/h3>\n\n\n\n<ol>\n<li>Go to the <a href=\"https:\/\/console.cloud.google.com\/functions\/add?\"><strong>Create a<\/strong> <strong>Google Cloud Function <\/strong>option menu<\/a>.<\/li>\n\n\n\n<li>Enter the function <strong>name <\/strong>and preferred <strong>region<\/strong>.<\/li>\n\n\n\n<li>For <strong>Trigger <\/strong>type, choose <strong>HTTP<\/strong>.<\/li>\n\n\n\n<li>For <strong>Authentication<\/strong>, choose <strong>Allow unauthenticated&nbsp;access<\/strong>. For more security, you can also choose <strong>Require authentication <\/strong>but this will restrict your function in terms of accessibility. We suggest going with Allow unauthenticated access for testing purposes and changing it later to restricted.<\/li>\n\n\n\n<li>Click on <strong>Advanced<\/strong>&nbsp;and specify the configurations,<br><img decoding=\"async\" src=\"https:\/\/lh3.googleusercontent.com\/zs-rBj6BO7y7Ehbim37ni5NLxXHZPItoDwG_jHgEPqU39it-4a8wxJnysWqeHN2SBYDX9e55sRe4z3ba3gqlasobsO82175ve3a0Of1LDOfUDuBZY-4SJkyXrcMStA\" width=\"576\" height=\"472\"><\/li>\n<\/ol>\n\n\n\n<ol>\n<li><strong>Memory Allocated: <\/strong>512 MB (Recommended)<\/li>\n\n\n\n<li><strong>Timeout: <\/strong>540 seconds (Recommended)<\/li>\n<\/ol>\n\n\n\n<p>6. Click on <strong>Variables, Networking and Advanced Settings <\/strong>to reveal more options. Here, you will create <strong>runtime environment variables <\/strong>which are variables accessible to your function using the <strong>os <\/strong>library at runtime. We are going to use the components of code that need to be changed i.e. the Cloud Storage paths for our credential files as well as the name of our project,<\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/lh6.googleusercontent.com\/6Onyqup9wBVCRpeQuGGiLcFDMnleBLJO_IVIlPdiApBYvfLy4h1kN-Dpp7z4yhPhmv6JPmTwBMtRImDWJJ-PIjygYZPuZY-sUnP0ct5BxCaozM4s1VzFvbtPf7XsAA\" alt=\"\"\/><\/figure>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td>bigquery_credentials: <br>&lt;path_to_file&gt;\/bigquery_credentials.json<br>google_credentials: &lt;path_to_file&gt;\/google_credentials.json<br>slack_webhook: &lt;path_to_file&gt;\/slack_webhook.json<br>project_name: &lt;project_name&gt;<\/td><\/tr><\/tbody><\/table><\/figure>\n\n\n\n<p>7. Once you are done, click on <strong>NEXT<\/strong>.<\/p>\n\n\n\n<p>This will create your function and take you to the <strong>Source <\/strong>page where you can edit and add the code. There are two essential files you need to upload here,<\/p>\n\n\n\n<ul>\n<li><strong>main.py &#8211;<\/strong>&nbsp;Contains the Python script.<\/li>\n\n\n\n<li><strong>requirements.txt &#8211; <\/strong>Contains the packages required by the main.py script.<\/li>\n<\/ul>\n\n\n\n<p>Both of these files are provided over <a href=\"https:\/\/gitlab.com\/-\/snippets\/2060409\">here<\/a>.<\/p>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Google Cloud Scheduler<\/strong><\/h2>\n\n\n\n<p>The final component is the Cloud Scheduler which is responsible for calling the Cloud Function. Here is how you can set it up,<\/p>\n\n\n\n<ol>\n<li>Open the <a href=\"https:\/\/console.cloud.google.com\/cloudscheduler\">Cloud Scheduler console<\/a>.<\/li>\n\n\n\n<li>Click on <strong>Create Job<\/strong>.<\/li>\n\n\n\n<li>Provide a <strong>Description<\/strong>.<\/li>\n\n\n\n<li>Provide a <strong>Frequency <\/strong>in <em>unix-cron <\/em>format and select the <strong>timezone<\/strong>. In our case, we will be running it on a daily basis at 9 AM so we have specified it as \u201c0 9 * * *\u201d.<br><br>You can use this <a href=\"https:\/\/crontab.guru\/\">simple editor<\/a> to figure out the schedule you want to run it on.<br><br><strong><em>Note:&nbsp;<\/em><\/strong><em>Do keep in mind that if you plan to do it on a different frequency, you will need to modify the code as it is currently designed to fetch only last day\u2019s data.<\/em><\/li>\n\n\n\n<li>Choose the <strong>Target <\/strong>as <em>HTTP<\/em>.<\/li>\n\n\n\n<li>Provide the <strong>URL <\/strong>of your function. This can be found in the <strong>Trigger <\/strong>tab of your Cloud Function.<\/li>\n<\/ol>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/lh5.googleusercontent.com\/Ft4KqSAwVWdDFSA0hsdau3Kh2NIWOiQQwKRsLutm-zyLLKkwZ9GN3y6EKeLEHi6J0UKwIbB_ZJDVXirevn8fa0qmneXfvxltKUpN8-UB-zDlrHM_WgFhFVn0ewYK8A\" alt=\"\"\/><\/figure>\n\n\n\n<ol start=\"7\">\n<li>Choose the <strong>HTTP method <\/strong>as <em>GET.<\/em><\/li>\n\n\n\n<li>Click on <strong>Create<\/strong>.<\/li>\n<\/ol>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/lh4.googleusercontent.com\/FgGX8pHq9bOhUHD165NIkTC0cuBsGKMEsUx29fs-0OP7PotROyyiDCnmG26UKzHrTEs7lhAQW0gtSiRAPD8q6-aAW-yGmCHhgTOZA1ICtzwb-3bl50sthoB0V4HQ1A\" alt=\"\"\/><\/figure>\n\n\n\n<p>Once the job is executed, you should see the following rows in your BigQuery table,<\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/lh5.googleusercontent.com\/gVezCClfVuG9aMP4WykxFO1Gjun_vbzRLjpMNsa2LnixmeMDafvbL9cG6TFHg--T7HoHsNiVeaS5qdSClJ64AMFGhnzPKOpySUS1i10iNzAMh0Mh71KLubolkDQwTw\" alt=\"\"\/><\/figure>\n\n\n\n<h1 class=\"wp-block-heading\"><strong>Code Architecture<\/strong><\/h1>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Workflow<\/strong><\/h2>\n\n\n\n<p><strong>main.py&nbsp;<\/strong>is the code file that contains the Python script. We have provided a basic workflow of how this works below,<br><\/p>\n\n\n\n<ol>\n<li>The function is triggered using a GET request through the Google Cloud Scheduler on a daily basis at a specified time (for more info, scroll down to the trigger section).<\/li>\n\n\n\n<li>When the function receives the request, it executes the code at the entry point i.e. <strong>hello_world <\/strong>function. Here it checks if it received a particular search query, this option has been added to backfill and extract Gmail messages for a particular date. However, for the daily job, this functionality is not required and if the function does not receive a <strong>query&nbsp;<\/strong>parameter in the GET request, it will create its own query for the previous Date. For example, if the function is executed on 23rd December GMT time, it will create the following search query,<br><br>after:2020\/12\/22 before:2020\/01\/23<br><\/li>\n\n\n\n<li>It will load Google credentials and create a <strong>service <\/strong>object and verify connectivity by pulling labels from Gmail\u2019s server.&nbsp;<\/li>\n\n\n\n<li>Next, it loads BigQuery credentials and creates a <strong>client <\/strong>object that is able to communicate with BigQuery.<\/li>\n\n\n\n<li>The table schema is created and formatted. For a batch upload job, we need to provide the data as well as the schema.<\/li>\n<\/ol>\n\n\n\n<p>Finally, we execute the main loop in which we access all the emails based on our provided query using the <a href=\"https:\/\/developers.google.com\/gmail\/api\/reference\/rest\/v1\/users.messages\/list\"><strong>users.messages.list method<\/strong><\/a><strong> <\/strong>exposed by Gmail\u2019s REST API.<\/p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td>messages=service.users().messages().list(userId=&#8217;me&#8217;,q=search_query,maxResults=maxMessages).execute()<\/td><\/tr><\/tbody><\/table><\/figure>\n\n\n\n<h2 class=\"wp-block-heading\"><strong><br><\/strong><strong>Batching<\/strong><\/h2>\n\n\n\n<p>Using the query \u201cafter:2020\/12\/22 before:2020\/01\/23\u201d, we can access all emails after 22nd December and before 23rd December. Since the exact number of emails between the search criteria is not known and we can access only a limited number of emails at a time, we have implemented batching.<\/p>\n\n\n\n<p>The batch size is controlled by the <strong>maxMessages <\/strong>variable which is set to 100 by default. The function will query Gmail for 100 messages and will upload these to BigQuery before proceeding with the next 100 pages. The <strong>nextPageToken <\/strong>parameter retrieved in the batch response is used for pagination. In case of any error, an alert is generated to Slack using the provided webhook URL.<\/p>\n\n\n\n<p>After each batch is uploaded, we compare the Total Uploaded count and the Total Retrieved count to tally the numbers. In case the numbers don\u2019t match, an alert is generated to Slack. All the Logging is done through a custom logging function that logs the event, the event type, and the time of the event.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>We love data. The more of it, the better. While our work is focused on digital analytics, using customer data to analyze their patterns and behaviors, this year, we decide to take it one step further and focus on our own patterns and behaviors. We realized that we have a wealth of data on how [&hellip;]<\/p>\n","protected":false},"author":5,"featured_media":6767,"comment_status":"closed","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"content-type":"","footnotes":""},"categories":[3,156],"tags":[],"acf":[],"aioseo_notices":[],"_links":{"self":[{"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/posts\/6765"}],"collection":[{"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/users\/5"}],"replies":[{"embeddable":true,"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/comments?post=6765"}],"version-history":[{"count":0,"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/posts\/6765\/revisions"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/media\/6767"}],"wp:attachment":[{"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/media?parent=6765"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/categories?post=6765"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/marketlytics.com\/wp-json\/wp\/v2\/tags?post=6765"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}